\documentclass{standalone}

\begin{document}
	\subsection{Flaws in Analysis}
	The greatest issue for the experiments is that Matlab does not allow for the user to change optimization levels or specifiy a global working precision. The default precision is double and if an elementary operation occurs such that one operand is single and the other is double, the former is first converted to double precision and then the result is converted back into single precision. This has the obvious issue of not being the same operation as having both operands be in single precision. Thus, a preprocessing step of casting all initial values such as the starting point and the coefficients of the function being evaluated to single precision has been done. The assumption here is that if all operands are initialized as single then a program should work as single and Matlab will take care of the rest of the type casts.
	\par
	Yet, this does not solve the issue completely since there is no way to have control over the optimizations the Matlab interpreter may make. For example, Matlab may introduce temporary variables that are in double precision under the hood or decide to use the extended precision registers available with x$86$ architecture machines (and the machine used for the experiments falls under this category). Another issue is that when evaluating the EFTs such as the \texttt{TwoSum} algorithm given in \ref{appendix:twosum} a smart interpreter/compiler may realize that the line \texttt{left-(val-s)} is equivalent to \texttt{left-(val-val+left)} which is then equivalent to \texttt{0} and consequently never execute this instruction. However, such relations that hold true over infinite precision are not generally correct for floating point computations and thus the optimization should not be made. It appears that Matlab is not doing this but this has not been verified. Finer points regarding issues when verifiying floating points computations can be found in \cite{monniaux2008pitfalls}.
	\par
	Criticisms regarding the experimental evaluation have already been given in Section~\ref{sec:analysis}. Further work could involve considering more types of functions (especially polynomials), run-time analysis to obtain a benefit-to-overhead understanding and an exploration of other bounds Langlois introduces in \cite{langlois2001automatic} to better understand how well the correction being made is.
	\subsection{Evaluation of CENA}
	While not discussed in this project, the error analysis of the CENA method (or methods) is tightly intertwined with the conditioning of a problem. \cite{langlois2001automatic} \cite{langlois2004more} This is naturally the case since errors are computed rather than formally approximated or estimated in some other way. For example, consider another work by Langlois which deals with solving triangular systems using the substitution algorithm. \cite{langlois2005solving} The paper considers systems of size $n\times n$ which become very ill-conditioned as $n$ increases. The relative forward error in the corrected single-precision solution also increases as the conditioning gets worse. This means that the effectiveness of the correction decreases as the need for sound corrections increases. This is also applicable to the corrected Newton's method algorithm since as the degree of the polynomial increases the accuracy of the corrected result decreases. Overall, it seems that the greatest advantage of CENA corrections is also its greatest weakness since errors are not approximated or reasoned about but instead computed. Thus, the CENA corrected Newton's method appears to be useful as possibly an optional parameter that could be activated for numerical software implementing Newton's method where a warning indicating ill-conditioning would also be necessary. Perhaps a more exciting direction would be that it could be used as an educational tool to demonstrate issues concering rounding error and the ideas behind methods that attempt to compensate forward error. Yet, CENA corrections should not be used for critical applications where the accuracy of a result is vital and instead other options including doubling the working precision should be considered.
\end{document}